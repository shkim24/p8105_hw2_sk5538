HW2
================
Senna
2024-09-30

Necessary packages are loaded.

``` r
library(tidyverse)
library(readxl)
```

## Problem 1

Read and clean data.

``` r
mta_df <- read_csv(file = "./data/NYC_Transit_Subway_Entrance_And_Exit_data.csv") |>
  janitor::clean_names() |>
  mutate(entry = ifelse(entry == "YES", TRUE, FALSE)) |>
  mutate(across(route1:route11, as.character)) |>
  pivot_longer(
    cols = route1:route11,
    names_to = 'route_index',
    names_prefix = 'route',
    values_to = 'route'
  )|>
  drop_na(route)|>
  distinct(line, station_name, station_latitude, station_longitude, route_index,route, entry, vending, entrance_type, ada)

head(mta_df)
```

    ## # A tibble: 6 × 10
    ##   line   station_name station_latitude station_longitude route_index route entry
    ##   <chr>  <chr>                   <dbl>             <dbl> <chr>       <chr> <lgl>
    ## 1 4 Ave… 25th St                  40.7             -74.0 1           R     TRUE 
    ## 2 4 Ave… 36th St                  40.7             -74.0 1           N     TRUE 
    ## 3 4 Ave… 36th St                  40.7             -74.0 2           R     TRUE 
    ## 4 4 Ave… 45th St                  40.6             -74.0 1           R     TRUE 
    ## 5 4 Ave… 53rd St                  40.6             -74.0 1           R     TRUE 
    ## 6 4 Ave… 53rd St                  40.6             -74.0 1           R     FALSE
    ## # ℹ 3 more variables: vending <chr>, entrance_type <chr>, ada <lgl>

First and foremost, the names of the variables are cleaned and the
‘entry’ variable is turned into a logical variable.

With just this, the dataset is not tidy as it has information on routes
spread across 11 columns. This is because the stations serve different
numbers of routes ranging from 1 to 11. So, the 11 columns were combined
into route and route index for each station, dropping any rows with NA
in the route column (e.g. Now, if a station serves 2 routes, the route
index would index the two routes served 1 and 2 respectively). Relevant
variables are retained with no duplicates. The dataset is now tidy.

It contains relevant variables of line, station_name, station_latitude,
station_longitude, route_index, route, entry, vending, entrance_type,
ada, and the dimensions are 1559, 10

``` r
distinct_station = distinct(mta_df, line, station_name)

ada_stations = mta_df |>
  filter(ada == TRUE) |>
  distinct (station_name, line)
```

There are 465 distinct stations, and 84 distinct stations are ADA
compliant

What proportion of station entrances / exits without vending allow
entrance?

``` r
no_vending_entrances <- mta_df |>
  filter(vending == "NO") |>
  summarise(proportion = mean(entry))
print(no_vending_entrances)
```

    ## # A tibble: 1 × 1
    ##   proportion
    ##        <dbl>
    ## 1      0.321

Reformat data so that route number and route name are distinct
variables.

``` r
mta_new <- mta_df |>
  mutate(
    route_name = ifelse(is.na(as.numeric(route)), route, NA),
    route_number = ifelse(!is.na(as.numeric(route)), as.numeric(route), NA)
  )|>
  select(
    line, station_name, route_index, route_name, route_number, everything()
  )
```

    ## Warning: There were 3 warnings in `mutate()`.
    ## The first warning was:
    ## ℹ In argument: `route_name = ifelse(is.na(as.numeric(route)), route, NA)`.
    ## Caused by warning in `ifelse()`:
    ## ! NAs introduced by coercion
    ## ℹ Run `dplyr::last_dplyr_warnings()` to see the 2 remaining warnings.

``` r
head(mta_new)
```

    ## # A tibble: 6 × 12
    ##   line     station_name route_index route_name route_number station_latitude
    ##   <chr>    <chr>        <chr>       <chr>             <dbl>            <dbl>
    ## 1 4 Avenue 25th St      1           R                    NA             40.7
    ## 2 4 Avenue 36th St      1           N                    NA             40.7
    ## 3 4 Avenue 36th St      2           R                    NA             40.7
    ## 4 4 Avenue 45th St      1           R                    NA             40.6
    ## 5 4 Avenue 53rd St      1           R                    NA             40.6
    ## 6 4 Avenue 53rd St      1           R                    NA             40.6
    ## # ℹ 6 more variables: station_longitude <dbl>, route <chr>, entry <lgl>,
    ## #   vending <chr>, entrance_type <chr>, ada <lgl>

``` r
A_train = mta_new |>
  filter (route_name =="A")

ada_A_train = A_train|>
  filter(ada == TRUE)
```

60 distinct stations serve the A train. Of the stations that serve the A
train, 17 are ADA compliant.

## Problem 2

``` r
mr_trashwheel= read_excel("./data/202409 Trash Wheel Collection Data.xlsx",
                          sheet=1, 
                          skip=1, 
                          na = c(".", "NA", ""),
                          )|>
  janitor::clean_names()|>
  select(-x15,-x16)
```

Omit rows that do not include dumpster-specific data.  
Round the number of sports balls to the nearest integer and converts the
result to an integer variable.

``` r
mr_trashwheel = mr_trashwheel|>
  drop_na(dumpster)|>
  mutate (sports_balls = as.integer(round(sports_balls)))
```

Import, clean, and organize the data for Professor Trash Wheel and
Gwynnda. And combine this with the Mr. Trash Wheel dataset to produce a
single tidy dataset.Keep track of which Trash Wheel is which.

``` r
professor_trashwheel= read_excel("./data/202409 Trash Wheel Collection Data.xlsx",
                          sheet=2, 
                          skip=1, 
                          na = c(".", "NA", ""),
                          )|>
  janitor::clean_names()|>
  mutate(trashwheel = "professor trashwheel")|>
  drop_na(dumpster)


gwynnda= read_excel("./data/202409 Trash Wheel Collection Data.xlsx",
                          sheet=4, 
                          skip=1, 
                          na = c(".", "NA", ""),
                          )|>
  janitor::clean_names()|>
  mutate(trashwheel = "gwynnda trashwheel")|>
  drop_na(dumpster)

mr_trashwheel = mr_trashwheel|>
  mutate(year = as.numeric(year),
         trashwheel = "mr trashwheel")
  

trashwheel_df <- bind_rows(
  mr_trashwheel,
  professor_trashwheel,
  gwynnda
)
```

Write a paragraph about these data; you are encouraged to use inline R.
Be sure to note the number of observations in the resulting dataset, and
give examples of key variables.

``` r
weight_professor = trashwheel_df|>
  filter(trashwheel == 'professor trashwheel')|>
  summarize (total_weight = sum(weight_tons))


cigarette_gwynnda = trashwheel_df|>
  filter(trashwheel == 'gwynnda trashwheel', year == 2022, month == "June")|>
  summarize(cigarette = sum(cigarette_butts))
```

The total weight of trash collected by Professor Trash Wheel is NA, and
the total number of cigarette butts collected by Gwynnda in June of 2022
is 1.812^{4}.

## Problem 3

Import, clean, tidy, and otherwise wrangle each of these datasets.

``` r
bakers = read_csv(file = './data/gbb_datasets/bakers.csv') |>
  janitor::clean_names()

bakes = read_csv(file = './data/gbb_datasets/bakes.csv', na = "N/A")|>
  janitor::clean_names()

results = read_csv(file = './data/gbb_datasets/results.csv', skip=2)|>
  janitor::clean_names()
```

check for completeness and correctness across datasets (e.g. by viewing
individual datasets and using anti_join);

``` r
bakes_results = bakes|>
  full_join (results, by = c('baker','series','episode'))

bakers = bakers|>
  mutate(baker = word(baker_name,1))|>
  select(series, baker, everything())

full_df = bakers|>
  full_join(bakes_results, by = c('baker','series'))|>
  select(baker_name, series, episode, result, technical, signature_bake, show_stopper, everything(), -baker)|>
  drop_na(result)|>
  arrange(series, baker_name)

head(full_df)
```

    ## # A tibble: 6 × 10
    ##   baker_name     series episode result technical signature_bake     show_stopper
    ##   <chr>           <dbl>   <dbl> <chr>      <dbl> <chr>              <chr>       
    ## 1 Annetha Mills       1       1 IN             2 Light Jamaican Bl… "Red, White…
    ## 2 Annetha Mills       1       2 OUT            7 Rose Petal Shortb… "Pink Swirl…
    ## 3 David Chambers      1       1 IN             3 Chocolate Orange … "Black Fore…
    ## 4 David Chambers      1       2 IN             8 Cheddar Cheese an… "Choux Past…
    ## 5 David Chambers      1       3 IN             4 Chilli Bread       "Walnut and…
    ## 6 David Chambers      1       4 OUT            5 Pear and Walnut P… "Apple and …
    ## # ℹ 3 more variables: baker_age <dbl>, baker_occupation <chr>, hometown <chr>

Export the result as a CSV in the directory containing the original
datasets.

``` r
write.csv(full_df, "./data/gbb_datasets/great_british_bake_off.csv", row.names = FALSE)
```

Describe your data cleaning process, including any questions you have or
choices you made. Briefly discuss the final dataset.

After skipping unnecessary rows, cleaning the variable names,and
handling any missing values, I first joined the bakes dataset and the
result dataset as they had a lot of variables in common. I used
full_join by the baker’s name, series, and episode to avoid problems
regarding more than one people having the same name.

In order to join the bakes_results joined dataset with the bakers
dataset by the bakers’ names, names on the bakers dataset first had to
be modified. It had full names while bakes and results dataset had first
names only. I extracted the first names from the bakers dataset
‘baker_name’ and used that to join the dataset with bakes_results. For
the fully merged dataset, I decided to retain the full name instead of
the first names to same name issues.

After merging the datasets, I dropped rows with result == NA in order to
remove unnecessary rows corresponding to after a baker is eiliminated
from the show. I then reordered the variables and arranged the rows to
make it more readable.

In the final dataset, the bakers and their result, score, signature
bake, and show stopper of each episode are shown in the order of the
series. Within the series, the bakers are organized in alphabetial
order.

Create a reader-friendly table showing the star baker or winner of each
episode in Seasons 5 through 10. Comment on this table – were there any
predictable overall winners? Any surprises?

``` r
winner_df = full_df|>
  filter (series >= 5 & series <= 10, result == 'WINNER'| result =='STAR BAKER')|>
  arrange(series, episode)|>
  select(series, episode, result, baker_name, everything())

head(winner_df)
```

    ## # A tibble: 6 × 10
    ##   series episode result     baker_name     technical signature_bake show_stopper
    ##    <dbl>   <dbl> <chr>      <chr>              <dbl> <chr>          <chr>       
    ## 1      5       1 STAR BAKER Nancy Birtwhi…         1 Coffee and Ha… Jaffa Orang…
    ## 2      5       2 STAR BAKER Richard Burr           1 Rosemary Seed… Pirates!    
    ## 3      5       3 STAR BAKER Luis Troyano           2 Opposites Att… Roscón de R…
    ## 4      5       4 STAR BAKER Richard Burr           5 Black Forest … Tiramisu Ba…
    ## 5      5       5 STAR BAKER Kate Henry             3 Rhubarb and C… Rhubarb, Pr…
    ## 6      5       6 STAR BAKER Chetna Makan           2 Orange Savari… Almond Liqu…
    ## # ℹ 3 more variables: baker_age <dbl>, baker_occupation <chr>, hometown <chr>

(in the last episode, it is unusual that David won the final round
without haveing any star baker throughout the season)

``` r
viewers = read_csv(file = './data/gbb_datasets/viewers.csv') |>
  janitor::clean_names()

head(viewers, 10)
```

    ## # A tibble: 10 × 11
    ##    episode series_1 series_2 series_3 series_4 series_5 series_6 series_7
    ##      <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>
    ##  1       1     2.24     3.1      3.85     6.6      8.51     11.6     13.6
    ##  2       2     3        3.53     4.6      6.65     8.79     11.6     13.4
    ##  3       3     3        3.82     4.53     7.17     9.28     12.0     13.0
    ##  4       4     2.6      3.6      4.71     6.82    10.2      12.4     13.3
    ##  5       5     3.03     3.83     4.61     6.95     9.95     12.4     13.1
    ##  6       6     2.75     4.25     4.82     7.32    10.1      12       13.1
    ##  7       7    NA        4.42     5.1      7.76    10.3      12.4     13.4
    ##  8       8    NA        5.06     5.35     7.41     9.02     11.1     13.3
    ##  9       9    NA       NA        5.7      7.41    10.7      12.6     13.4
    ## 10      10    NA       NA        6.74     9.45    13.5      15.0     15.9
    ## # ℹ 3 more variables: series_8 <dbl>, series_9 <dbl>, series_10 <dbl>

``` r
mean_view_1 = mean(viewers[["series_1"]], na.rm=TRUE)
mean_view_5 = mean(viewers[["series_5"]], na.rm=TRUE)
```

The average viewership in season 1 was 2.77. That in season 5 was
10.0393.
