HW2
================
Senna
2024-09-30

Necessary packages are loaded.

``` r
library(tidyverse)
library(readxl)
options(scipen = 999)
```

## Problem 1

Read and clean data.

``` r
mta_df <- read_csv(file = "./data/NYC_Transit_Subway_Entrance_And_Exit_data.csv") |>
  janitor::clean_names() |>
  mutate(entry = ifelse(entry == "YES", TRUE, FALSE)) |>
  mutate(across(route1:route11, as.character)) |>
  pivot_longer(
    cols = route1:route11,
    names_to = 'route_index',
    names_prefix = 'route',
    values_to = 'route'
  )|>
  drop_na(route)|>
  distinct(line, station_name, station_latitude, station_longitude, route_index,route, entry, vending, entrance_type, ada)

head(mta_df)
```

    ## # A tibble: 6 × 10
    ##   line   station_name station_latitude station_longitude route_index route entry
    ##   <chr>  <chr>                   <dbl>             <dbl> <chr>       <chr> <lgl>
    ## 1 4 Ave… 25th St                  40.7             -74.0 1           R     TRUE 
    ## 2 4 Ave… 36th St                  40.7             -74.0 1           N     TRUE 
    ## 3 4 Ave… 36th St                  40.7             -74.0 2           R     TRUE 
    ## 4 4 Ave… 45th St                  40.6             -74.0 1           R     TRUE 
    ## 5 4 Ave… 53rd St                  40.6             -74.0 1           R     TRUE 
    ## 6 4 Ave… 53rd St                  40.6             -74.0 1           R     FALSE
    ## # ℹ 3 more variables: vending <chr>, entrance_type <chr>, ada <lgl>

First, the names of the variables are cleaned and the ‘entry’ variable
is turned into a logical variable as instructed.

Afte this step, the dataset is not tidy as it has information on routes
spread across 11 columns. This is because the stations serve different
numbers of routes ranging from 1 to 11. So, the 11 columns were combined
into route and route index for each station, dropping any rows with NA
in the route column (e.g. Now, if a station serves 2 routes, the route
index would index the two routes served 1 and 2 respectively). Relevant
variables are retained with no duplicates. The dataset is now tidy.

It contains relevant variables of line, station_name, station_latitude,
station_longitude, route_index, route, entry, vending, entrance_type,
ada, and the dimensions are 1559, 10

``` r
distinct_station = distinct(mta_df, line, station_name)

ada_stations = mta_df |>
  filter(ada == TRUE) |>
  distinct (station_name, line)
```

There are 465 distinct stations, and 84 distinct stations are ADA
compliant

``` r
no_vending_entrances <- mta_df |>
  filter(vending == "NO") |>
  summarise(proportion = mean(entry))
```

The proportion of station entrances / exits without vending is 0.3212996

Reformat data so that route number and route name are distinct
variables.

``` r
mta_new <- mta_df |>
  mutate(
    route_name = ifelse(is.na(as.numeric(route)), route, NA),
    route_number = ifelse(!is.na(as.numeric(route)), as.numeric(route), NA)
  )|>
  select(
    line, station_name, route_index, route_name, route_number, everything()
  )
```

    ## Warning: There were 3 warnings in `mutate()`.
    ## The first warning was:
    ## ℹ In argument: `route_name = ifelse(is.na(as.numeric(route)), route, NA)`.
    ## Caused by warning in `ifelse()`:
    ## ! NAs introduced by coercion
    ## ℹ Run `dplyr::last_dplyr_warnings()` to see the 2 remaining warnings.

``` r
head(mta_new)
```

    ## # A tibble: 6 × 12
    ##   line     station_name route_index route_name route_number station_latitude
    ##   <chr>    <chr>        <chr>       <chr>             <dbl>            <dbl>
    ## 1 4 Avenue 25th St      1           R                    NA             40.7
    ## 2 4 Avenue 36th St      1           N                    NA             40.7
    ## 3 4 Avenue 36th St      2           R                    NA             40.7
    ## 4 4 Avenue 45th St      1           R                    NA             40.6
    ## 5 4 Avenue 53rd St      1           R                    NA             40.6
    ## 6 4 Avenue 53rd St      1           R                    NA             40.6
    ## # ℹ 6 more variables: station_longitude <dbl>, route <chr>, entry <lgl>,
    ## #   vending <chr>, entrance_type <chr>, ada <lgl>

``` r
A_train = mta_new |>
  filter (route_name =="A")

ada_A_train = A_train|>
  filter(ada == TRUE)
```

60 distinct stations serve the A train. Of the stations that serve the A
train, 17 are ADA compliant.

## Problem 2

``` r
mr_trashwheel= read_excel("./data/202409 Trash Wheel Collection Data.xlsx",
                          sheet=1, 
                          skip=1, 
                          na = c(".", "NA", ""),
                          )|>
  janitor::clean_names()|>
  select(-x15,-x16)
```

Omit rows that do not include dumpster-specific data.  
Round the number of sports balls to the nearest integer and converts the
result to an integer variable.

``` r
mr_trashwheel = mr_trashwheel|>
  drop_na(dumpster)|>
  mutate (sports_balls = as.integer(round(sports_balls)))
```

Import, clean, and organize the data for Professor Trash Wheel and
Gwynnda. And combine this with the Mr. Trash Wheel dataset to produce a
single tidy dataset.Keep track of which Trash Wheel is which.

``` r
professor_trashwheel= read_excel("./data/202409 Trash Wheel Collection Data.xlsx",
                          sheet=2, 
                          skip=1, 
                          na = c(".", "NA", ""),
                          )|>
  janitor::clean_names()|>
  mutate(trashwheel = "professor trashwheel")|>
  drop_na(dumpster)


gwynnda= read_excel("./data/202409 Trash Wheel Collection Data.xlsx",
                          sheet=4, 
                          skip=1, 
                          na = c(".", "NA", ""),
                          )|>
  janitor::clean_names()|>
  mutate(trashwheel = "gwynnda trashwheel")|>
  drop_na(dumpster)

mr_trashwheel = mr_trashwheel|>
  mutate(year = as.numeric(year),
         trashwheel = "mr trashwheel")
  

trashwheel_df = bind_rows(
  mr_trashwheel,
  professor_trashwheel,
  gwynnda
)
```

There is a total of 1033 observations in the resulting dataset. Some of
the key variables are the dumpster number, the date, and the amount of
trash collected measured in tons. Among the weight collected, the amount
of some types of trash is also shown, such as plastic bottles, cigarette
butts, plastic bags, and so on. The full list of variables are as
following dumpster, month, year, date, weight_tons, volume_cubic_yards,
plastic_bottles, polystyrene, cigarette_butts, glass_bottles,
plastic_bags, wrappers, sports_balls, homes_powered, trashwheel

``` r
weight_professor = trashwheel_df|>
  filter(trashwheel == 'professor trashwheel')|>
  summarize (total_weight = sum(weight_tons, na.rm=TRUE))


cigarette_gwynnda = trashwheel_df|>
  filter(trashwheel == 'gwynnda trashwheel', year == 2022, month == "June")|>
  summarize(cigarette = sum(cigarette_butts, na.rm=TRUE))
```

The total weight of trash collected by Professor Trash Wheel is 246.74
tons, and the total number of cigarette butts collected by Gwynnda in
June of 2022 is 18120.

## Problem 3

Import, clean, tidy, and otherwise wrangle each of these datasets.

``` r
bakers = read_csv(file = './data/gbb_datasets/bakers.csv') |>
  janitor::clean_names()

bakes = read_csv(file = './data/gbb_datasets/bakes.csv', na = "N/A")|>
  janitor::clean_names()

results = read_csv(file = './data/gbb_datasets/results.csv', skip=2)|>
  janitor::clean_names()
```

``` r
view(bakers)
view(bakes)
view(results)
anti_join(results, bakes, by='baker')
anti_join(bakers, bakes)
anti_join(bakers, results)
```

Merge the datasets and tidy it to be in a meaningful order.

``` r
bakes_results = bakes|>
  full_join (results, by = c('baker','series','episode'))

bakers = bakers|>
  mutate(baker = word(baker_name,1))|>
  select(series, baker, everything())

full_df = bakers|>
  full_join(bakes_results, by = c('baker','series'))|>
  select(baker_name, series, episode, result, technical, signature_bake, show_stopper, everything(), -baker)|>
  drop_na(result)|>
  arrange(series, baker_name)

head(full_df,10)
```

    ## # A tibble: 10 × 10
    ##    baker_name     series episode result technical signature_bake    show_stopper
    ##    <chr>           <dbl>   <dbl> <chr>      <dbl> <chr>             <chr>       
    ##  1 Annetha Mills       1       1 IN             2 Light Jamaican B… "Red, White…
    ##  2 Annetha Mills       1       2 OUT            7 Rose Petal Short… "Pink Swirl…
    ##  3 David Chambers      1       1 IN             3 Chocolate Orange… "Black Fore…
    ##  4 David Chambers      1       2 IN             8 Cheddar Cheese a… "Choux Past…
    ##  5 David Chambers      1       3 IN             4 Chilli Bread      "Walnut and…
    ##  6 David Chambers      1       4 OUT            5 Pear and Walnut … "Apple and …
    ##  7 Edd Kimber          1       1 IN             1 Caramel Cinnamon…  <NA>       
    ##  8 Edd Kimber          1       2 IN             6 Oatmeal Raisin C… "Pink Macar…
    ##  9 Edd Kimber          1       3 IN             1 Olive Bread       "Tomato and…
    ## 10 Edd Kimber          1       4 IN             3 Apple and Plum P… "Apple and …
    ## # ℹ 3 more variables: baker_age <dbl>, baker_occupation <chr>, hometown <chr>

Export the result as a CSV in the directory containing the original
datasets.

``` r
write.csv(full_df, "./data/gbb_datasets/great_british_bake_off.csv", row.names = FALSE)
```

After skipping unnecessary rows, cleaning the variable names,and
handling any missing values, I first joined the bakes dataset and the
result dataset as they had a lot of variables in common. I used
full_join by the baker’s name, series, and episode to avoid problems
regarding more than one people having the same first name.

In order to join the bakes+results joined dataset with the bakers
dataset by the bakers’ names, names on the bakers dataset first had to
be modified. It had full names while bakes and results dataset had first
names only. I extracted the first names from the bakers dataset
‘baker_name’ and used that to join the dataset with bakes_results. For
the fully merged dataset, I decided to retain the full name instead of
the first names to avoid same name issues.

After merging the datasets, I dropped rows with result == NA in order to
remove unnecessary rows corresponding to after a baker is eliminated
from the show. I then reordered the variables and arranged the rows to
make it more readable.

Overall, the final dataset is arranged by ascending series. Within each
series, the data is grouped by bakers’ names. The name, result, score,
signature bake, and show stopper are shown in order of the episodes the
baker was in, until they are eliminated from the show. Bakers’ names in
each series are arranged in alphabetical order.The information about
each baker (age, occupation, and hometown) are the last three columns,
because it is repetitive and not of high important regarding the show.

Create a reader-friendly table showing the star baker or winner of each
episode in Seasons 5 through 10.

``` r
winner_df = full_df|>
  filter (series >= 5 & series <= 10, result == 'WINNER'| result =='STAR BAKER')|>
  arrange(series, episode)|>
  select(series, episode, result, baker_name, everything())

head(winner_df,12)
```

    ## # A tibble: 12 × 10
    ##    series episode result     baker_name    technical signature_bake show_stopper
    ##     <dbl>   <dbl> <chr>      <chr>             <dbl> <chr>          <chr>       
    ##  1      5       1 STAR BAKER Nancy Birtwh…         1 Coffee and Ha… "Jaffa Oran…
    ##  2      5       2 STAR BAKER Richard Burr          1 Rosemary Seed… "Pirates!"  
    ##  3      5       3 STAR BAKER Luis Troyano          2 Opposites Att… "Roscón de …
    ##  4      5       4 STAR BAKER Richard Burr          5 Black Forest … "Tiramisu B…
    ##  5      5       5 STAR BAKER Kate Henry            3 Rhubarb and C… "Rhubarb, P…
    ##  6      5       6 STAR BAKER Chetna Makan          2 Orange Savari… "Almond Liq…
    ##  7      5       7 STAR BAKER Richard Burr          1 Minted Lamb P… "Stair of É…
    ##  8      5       8 STAR BAKER Richard Burr          4 Fruit Swedish… "Rhubarb an…
    ##  9      5       9 STAR BAKER Richard Burr          2 Rose and Pist… "Hazelnut M…
    ## 10      5      10 WINNER     Nancy Birtwh…         1 Apple and Lem… "Red Windmi…
    ## 11      6       1 STAR BAKER Marie Campbe…         3 Zingy Citrus … "A Walk in …
    ## 12      6       2 STAR BAKER Ian Cumming           3 Orange, Rosem… "Sandwich d…
    ## # ℹ 3 more variables: baker_age <dbl>, baker_occupation <chr>, hometown <chr>

From seasons 5 to 10, the winner of the season was a start baker in at
least one of the episodes in most cases. However, in season 10, it is
unique that David Atherton won the season without any prior star baker.

``` r
viewers = read_csv(file = './data/gbb_datasets/viewers.csv') |>
  janitor::clean_names()

head(viewers, 10)
```

    ## # A tibble: 10 × 11
    ##    episode series_1 series_2 series_3 series_4 series_5 series_6 series_7
    ##      <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>
    ##  1       1     2.24     3.1      3.85     6.6      8.51     11.6     13.6
    ##  2       2     3        3.53     4.6      6.65     8.79     11.6     13.4
    ##  3       3     3        3.82     4.53     7.17     9.28     12.0     13.0
    ##  4       4     2.6      3.6      4.71     6.82    10.2      12.4     13.3
    ##  5       5     3.03     3.83     4.61     6.95     9.95     12.4     13.1
    ##  6       6     2.75     4.25     4.82     7.32    10.1      12       13.1
    ##  7       7    NA        4.42     5.1      7.76    10.3      12.4     13.4
    ##  8       8    NA        5.06     5.35     7.41     9.02     11.1     13.3
    ##  9       9    NA       NA        5.7      7.41    10.7      12.6     13.4
    ## 10      10    NA       NA        6.74     9.45    13.5      15.0     15.9
    ## # ℹ 3 more variables: series_8 <dbl>, series_9 <dbl>, series_10 <dbl>

``` r
mean_view_1 = mean(viewers[["series_1"]], na.rm=TRUE)
mean_view_5 = mean(viewers[["series_5"]], na.rm=TRUE)
```

The average viewership in season 1 was 2.77. That in season 5 was
10.0393.
